{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for LLM Fine-Tuning\n",
    "\n",
    "This notebook demonstrates how to prepare datasets for fine-tuning language models. We'll cover:\n",
    "\n",
    "1. Data collection and curation\n",
    "2. Data cleaning and preprocessing\n",
    "3. Converting to supported formats (Alpaca and ShareGPT)\n",
    "4. Analyzing dataset statistics\n",
    "5. Splitting into training and evaluation sets\n",
    "6. Tokenization and formatting\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, we'll import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add the project root to path\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import project utilities\n",
    "from src.utils.data_processing import calculate_dataset_statistics\n",
    "\n",
    "# Create directories if they don't exist\n",
    "data_dir = project_root / \"data\"\n",
    "data_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Collection and Curation\n",
    "\n",
    "When fine-tuning an LLM, the quality of your training data is crucial. Here are some strategies for collecting high-quality data:\n",
    "\n",
    "1. **Manual creation**: Write examples that demonstrate desired behavior\n",
    "2. **Curated datasets**: Use existing datasets from sources like Hugging Face\n",
    "3. **Data synthesis**: Generate examples using another LLM\n",
    "4. **Real-world data**: Utilize conversations, documents, or interactions from your domain\n",
    "\n",
    "Let's look at some example data and discuss curation principles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine sample data\n",
    "sample_alpaca_path = data_dir / \"sample_alpaca.json\"\n",
    "\n",
    "with open(sample_alpaca_path, \"r\") as f:\n",
    "    sample_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(sample_data)} examples from {sample_alpaca_path}\")\n",
    "print(\"\\nSample example:\")\n",
    "print(json.dumps(sample_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Curation Principles\n",
    "\n",
    "When curating a dataset for fine-tuning, consider these important principles:\n",
    "\n",
    "1. **Diversity**: Include a wide range of examples that cover different aspects of the task\n",
    "2. **Quality**: Ensure responses are high-quality, accurate, and helpful\n",
    "3. **Balance**: Balance different types of tasks or domains in your dataset\n",
    "4. **Consistency**: Maintain consistent style, tone, and format across examples\n",
    "5. **Representativeness**: Examples should represent real-world use cases\n",
    "\n",
    "Let's create a simple function to help assess data quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(data: List[Dict[str, Any]], format_type: str = \"alpaca\"):\n",
    "    \"\"\"Assess basic quality metrics for a dataset.\"\"\"\n",
    "    issues = []\n",
    "    stats = {}\n",
    "    \n",
    "    if format_type == \"alpaca\":\n",
    "        # Check for empty fields\n",
    "        empty_instructions = [i for i, item in enumerate(data) if not item.get(\"instruction\", \"\").strip()]\n",
    "        empty_outputs = [i for i, item in enumerate(data) if not item.get(\"output\", \"\").strip()]\n",
    "        \n",
    "        # Check output length\n",
    "        output_lengths = [len(item.get(\"output\", \"\").split()) for item in data]\n",
    "        \n",
    "        # Check for very short outputs (potential low quality)\n",
    "        short_outputs = [i for i, item in enumerate(data) if len(item.get(\"output\", \"\").split()) < 10]\n",
    "        \n",
    "        # Check for very long outputs (potential issues)\n",
    "        long_outputs = [i for i, item in enumerate(data) if len(item.get(\"output\", \"\").split()) > 500]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats[\"empty_instructions\"] = len(empty_instructions)\n",
    "        stats[\"empty_outputs\"] = len(empty_outputs)\n",
    "        stats[\"short_outputs\"] = len(short_outputs)\n",
    "        stats[\"long_outputs\"] = len(long_outputs)\n",
    "        stats[\"avg_output_length\"] = np.mean(output_lengths)\n",
    "        stats[\"min_output_length\"] = min(output_lengths)\n",
    "        stats[\"max_output_length\"] = max(output_lengths)\n",
    "        \n",
    "        if empty_instructions:\n",
    "            issues.append(f\"Found {len(empty_instructions)} examples with empty instructions\")\n",
    "        if empty_outputs:\n",
    "            issues.append(f\"Found {len(empty_outputs)} examples with empty outputs\")\n",
    "        if short_outputs:\n",
    "            issues.append(f\"Found {len(short_outputs)} examples with very short outputs (<10 words)\")\n",
    "            \n",
    "    elif format_type == \"sharegpt\":\n",
    "        # Stats for ShareGPT format\n",
    "        conversation_lengths = [len(item.get(\"conversations\", [])) for item in data]\n",
    "        empty_convs = [i for i, item in enumerate(data) if not item.get(\"conversations\", [])]\n",
    "        \n",
    "        # Check for conversations with missing roles or content\n",
    "        invalid_msgs = []\n",
    "        for i, item in enumerate(data):\n",
    "            for j, msg in enumerate(item.get(\"conversations\", [])):\n",
    "                if not msg.get(\"role\", \"\") or not msg.get(\"value\", \"\"):\n",
    "                    invalid_msgs.append((i, j))\n",
    "        \n",
    "        stats[\"empty_conversations\"] = len(empty_convs)\n",
    "        stats[\"invalid_messages\"] = len(invalid_msgs)\n",
    "        stats[\"avg_conversation_length\"] = np.mean(conversation_lengths)\n",
    "        stats[\"min_conversation_length\"] = min(conversation_lengths) if conversation_lengths else 0\n",
    "        stats[\"max_conversation_length\"] = max(conversation_lengths) if conversation_lengths else 0\n",
    "        \n",
    "        if empty_convs:\n",
    "            issues.append(f\"Found {len(empty_convs)} examples with empty conversations\")\n",
    "        if invalid_msgs:\n",
    "            issues.append(f\"Found {len(invalid_msgs)} messages with missing role or content\")\n",
    "    \n",
    "    return {\"stats\": stats, \"issues\": issues}\n",
    "\n",
    "# Assess quality of our sample data\n",
    "quality_assessment = assess_data_quality(sample_data, \"alpaca\")\n",
    "print(\"Data Quality Assessment:\")\n",
    "print(\"\\nStatistics:\")\n",
    "for key, value in quality_assessment[\"stats\"].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nIssues:\")\n",
    "if quality_assessment[\"issues\"]:\n",
    "    for issue in quality_assessment[\"issues\"]:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"  No major issues found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing\n",
    "\n",
    "Before using the data for training, it's important to clean and preprocess it. Common preprocessing steps include:\n",
    "\n",
    "1. **Removing duplicates**: Eliminate duplicate examples\n",
    "2. **Fixing formatting issues**: Normalize text formatting\n",
    "3. **Filtering inappropriate content**: Remove harmful or inappropriate examples\n",
    "4. **Anonymizing data**: Remove personally identifiable information (PII)\n",
    "5. **Normalizing text**: Standardize whitespace, newlines, etc.\n",
    "\n",
    "Let's implement some basic cleaning functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Basic text cleaning function.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Fix extra whitespace\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Normalize newlines (preserve paragraph breaks)\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = \"\\n\\n\".join([para.strip() for para in text.split(\"\\n\\n\")])\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def remove_duplicates(data: List[Dict[str, Any]], format_type: str = \"alpaca\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Remove duplicate examples based on content.\"\"\"\n",
    "    if format_type == \"alpaca\":\n",
    "        seen = set()\n",
    "        unique_data = []\n",
    "        \n",
    "        for item in data:\n",
    "            # Create a key from instruction and input (if present)\n",
    "            key = (item.get(\"instruction\", \"\").strip(), item.get(\"input\", \"\").strip())\n",
    "            \n",
    "            if key not in seen and key[0]:  # Ensure instruction is not empty\n",
    "                seen.add(key)\n",
    "                unique_data.append(item)\n",
    "                \n",
    "        return unique_data\n",
    "    \n",
    "    elif format_type == \"sharegpt\":\n",
    "        # For ShareGPT, deduplication is more complex\n",
    "        # We'll use a simple heuristic based on the first user message\n",
    "        seen = set()\n",
    "        unique_data = []\n",
    "        \n",
    "        for item in data:\n",
    "            first_user_msg = \"\"\n",
    "            for msg in item.get(\"conversations\", []):\n",
    "                if msg.get(\"role\", \"\").lower() in [\"user\", \"human\"]:\n",
    "                    first_user_msg = msg.get(\"value\", \"\").strip()\n",
    "                    break\n",
    "            \n",
    "            if first_user_msg and first_user_msg not in seen:\n",
    "                seen.add(first_user_msg)\n",
    "                unique_data.append(item)\n",
    "                \n",
    "        return unique_data\n",
    "    \n",
    "    return data\n",
    "\n",
    "def clean_dataset(data: List[Dict[str, Any]], format_type: str = \"alpaca\") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Apply cleaning functions to a dataset.\"\"\"\n",
    "    cleaned_data = []\n",
    "    \n",
    "    if format_type == \"alpaca\":\n",
    "        for item in data:\n",
    "            cleaned_item = {\n",
    "                \"instruction\": clean_text(item.get(\"instruction\", \"\")),\n",
    "                \"input\": clean_text(item.get(\"input\", \"\")),\n",
    "                \"output\": clean_text(item.get(\"output\", \"\"))\n",
    "            }\n",
    "            \n",
    "            # Only include examples with non-empty instruction and output\n",
    "            if cleaned_item[\"instruction\"] and cleaned_item[\"output\"]:\n",
    "                cleaned_data.append(cleaned_item)\n",
    "    \n",
    "    elif format_type == \"sharegpt\":\n",
    "        for item in data:\n",
    "            cleaned_conversations = []\n",
    "            \n",
    "            for msg in item.get(\"conversations\", []):\n",
    "                if msg.get(\"role\") and msg.get(\"value\"):\n",
    "                    cleaned_conversations.append({\n",
    "                        \"role\": msg[\"role\"],\n",
    "                        \"value\": clean_text(msg[\"value\"])\n",
    "                    })\n",
    "            \n",
    "            if cleaned_conversations:  # Only include non-empty conversations\n",
    "                cleaned_data.append({\"conversations\": cleaned_conversations})\n",
    "    \n",
    "    # Remove duplicates\n",
    "    cleaned_data = remove_duplicates(cleaned_data, format_type)\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "# Clean our sample data\n",
    "cleaned_data = clean_dataset(sample_data, \"alpaca\")\n",
    "print(f\"Original data: {len(sample_data)} examples\")\n",
    "print(f\"Cleaned data: {len(cleaned_data)} examples\")\n",
    "\n",
    "# View a cleaned example\n",
    "print(\"\\nCleaned Example:\")\n",
    "print(json.dumps(cleaned_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Converting to Supported Formats\n",
    "\n",
    "Our framework primarily supports two formats: Alpaca and ShareGPT. Let's create functions to convert data to these formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_alpaca_format(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Convert data to Alpaca format (instruction, input, output).\"\"\"\n",
    "    alpaca_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Handle common formats\n",
    "        if \"instruction\" in item and \"output\" in item:\n",
    "            # Already in Alpaca format\n",
    "            alpaca_item = {\n",
    "                \"instruction\": item[\"instruction\"],\n",
    "                \"input\": item.get(\"input\", \"\"),\n",
    "                \"output\": item[\"output\"]\n",
    "            }\n",
    "            alpaca_data.append(alpaca_item)\n",
    "            \n",
    "        elif \"prompt\" in item and \"completion\" in item:\n",
    "            # Convert prompt-completion format\n",
    "            alpaca_item = {\n",
    "                \"instruction\": item[\"prompt\"],\n",
    "                \"input\": \"\",\n",
    "                \"output\": item[\"completion\"]\n",
    "            }\n",
    "            alpaca_data.append(alpaca_item)\n",
    "            \n",
    "        elif \"question\" in item and \"answer\" in item:\n",
    "            # Convert QA format\n",
    "            alpaca_item = {\n",
    "                \"instruction\": item[\"question\"],\n",
    "                \"input\": \"\",\n",
    "                \"output\": item[\"answer\"]\n",
    "            }\n",
    "            alpaca_data.append(alpaca_item)\n",
    "            \n",
    "        elif \"conversations\" in item:\n",
    "            # Convert from ShareGPT format\n",
    "            conversations = item[\"conversations\"]\n",
    "            \n",
    "            # Find first user message as instruction\n",
    "            instruction = \"\"\n",
    "            for msg in conversations:\n",
    "                if msg.get(\"role\", \"\").lower() in [\"user\", \"human\"]:\n",
    "                    instruction = msg.get(\"value\", \"\")\n",
    "                    break\n",
    "            \n",
    "            # Find first assistant response as output\n",
    "            output = \"\"\n",
    "            for msg in conversations:\n",
    "                if msg.get(\"role\", \"\").lower() in [\"assistant\", \"bot\", \"gpt\"]:\n",
    "                    output = msg.get(\"value\", \"\")\n",
    "                    break\n",
    "                    \n",
    "            if instruction and output:\n",
    "                alpaca_item = {\n",
    "                    \"instruction\": instruction,\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": output\n",
    "                }\n",
    "                alpaca_data.append(alpaca_item)\n",
    "    \n",
    "    return alpaca_data\n",
    "\n",
    "def convert_to_sharegpt_format(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Convert data to ShareGPT format (conversations with roles).\"\"\"\n",
    "    sharegpt_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        if \"conversations\" in item:\n",
    "            # Already in ShareGPT format\n",
    "            sharegpt_data.append(item)\n",
    "            \n",
    "        elif \"instruction\" in item and \"output\" in item:\n",
    "            # Convert from Alpaca format\n",
    "            conversations = []\n",
    "            \n",
    "            # Add system message if needed\n",
    "            system_msg = \"You are a helpful, harmless, and honest AI assistant.\"\n",
    "            conversations.append({\"role\": \"system\", \"value\": system_msg})\n",
    "            \n",
    "            # Add user message (combine instruction and input)\n",
    "            user_msg = item[\"instruction\"]\n",
    "            if item.get(\"input\", \"\"):\n",
    "                user_msg += \"\\n\\n\" + item[\"input\"]\n",
    "                \n",
    "            conversations.append({\"role\": \"human\", \"value\": user_msg})\n",
    "            \n",
    "            # Add assistant response\n",
    "            conversations.append({\"role\": \"assistant\", \"value\": item[\"output\"]})\n",
    "            \n",
    "            sharegpt_data.append({\"conversations\": conversations})\n",
    "            \n",
    "        elif \"prompt\" in item and \"completion\" in item:\n",
    "            # Convert prompt-completion format\n",
    "            conversations = [\n",
    "                {\"role\": \"system\", \"value\": \"You are a helpful, harmless, and honest AI assistant.\"},\n",
    "                {\"role\": \"human\", \"value\": item[\"prompt\"]},\n",
    "                {\"role\": \"assistant\", \"value\": item[\"completion\"]}\n",
    "            ]\n",
    "            sharegpt_data.append({\"conversations\": conversations})\n",
    "    \n",
    "    return sharegpt_data\n",
    "\n",
    "# Convert our sample data to ShareGPT format\n",
    "sharegpt_data = convert_to_sharegpt_format(cleaned_data)\n",
    "print(f\"Converted {len(sharegpt_data)} examples to ShareGPT format\")\n",
    "print(\"\\nShareGPT Example:\")\n",
    "print(json.dumps(sharegpt_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing Dataset Statistics\n",
    "\n",
    "Before using your dataset for training, it's important to understand its characteristics. Let's create some functions to analyze the dataset statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_alpaca_dataset(data: List[Dict[str, Any]]):\n",
    "    \"\"\"Analyze and visualize Alpaca format dataset statistics.\"\"\"\n",
    "    # Extract key statistics\n",
    "    instruction_lengths = [len(item[\"instruction\"].split()) for item in data]\n",
    "    input_lengths = [len(item.get(\"input\", \"\").split()) for item in data]\n",
    "    output_lengths = [len(item[\"output\"].split()) for item in data]\n",
    "    \n",
    "    # Count examples with inputs\n",
    "    has_input = sum(1 for item in data if item.get(\"input\", \"\").strip())\n",
    "    \n",
    "    # Print basic statistics\n",
    "    print(f\"Total examples: {len(data)}\")\n",
    "    print(f\"Examples with input field: {has_input} ({has_input/len(data)*100:.1f}%)\")\n",
    "    print(\"\\nLength statistics (in words):\")\n",
    "    print(f\"  Instruction: avg={np.mean(instruction_lengths):.1f}, min={min(instruction_lengths)}, max={max(instruction_lengths)}\")\n",
    "    print(f\"  Input: avg={np.mean(input_lengths):.1f}, min={min(input_lengths)}, max={max(input_lengths)}\")\n",
    "    print(f\"  Output: avg={np.mean(output_lengths):.1f}, min={min(output_lengths)}, max={max(output_lengths)}\")\n",
    "    \n",
    "    # Visualize length distributions\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Instruction length\n",
    "    sns.histplot(instruction_lengths, kde=True, ax=axes[0])\n",
    "    axes[0].set_title(\"Instruction Length Distribution\")\n",
    "    axes[0].set_xlabel(\"Word Count\")\n",
    "    \n",
    "    # Input length (for examples with input)\n",
    "    non_empty_inputs = [length for length in input_lengths if length > 0]\n",
    "    if non_empty_inputs:\n",
    "        sns.histplot(non_empty_inputs, kde=True, ax=axes[1])\n",
    "        axes[1].set_title(\"Input Length Distribution (non-empty)\")\n",
    "        axes[1].set_xlabel(\"Word Count\")\n",
    "    else:\n",
    "        axes[1].set_title(\"No examples with input field\")\n",
    "    \n",
    "    # Output length\n",
    "    sns.histplot(output_lengths, kde=True, ax=axes[2])\n",
    "    axes[2].set_title(\"Output Length Distribution\")\n",
    "    axes[2].set_xlabel(\"Word Count\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return statistics dict\n",
    "    return {\n",
    "        \"count\": len(data),\n",
    "        \"with_input\": has_input,\n",
    "        \"avg_instruction_length\": np.mean(instruction_lengths),\n",
    "        \"avg_input_length\": np.mean(input_lengths),\n",
    "        \"avg_output_length\": np.mean(output_lengths),\n",
    "    }\n",
    "\n",
    "def analyze_sharegpt_dataset(data: List[Dict[str, Any]]):\n",
    "    \"\"\"Analyze and visualize ShareGPT format dataset statistics.\"\"\"\n",
    "    # Extract key statistics\n",
    "    conversation_lengths = [len(item[\"conversations\"]) for item in data]\n",
    "    msg_lengths = []\n",
    "    role_counts = {}\n",
    "    \n",
    "    for item in data:\n",
    "        for msg in item[\"conversations\"]:\n",
    "            role = msg[\"role\"].lower()\n",
    "            role_counts[role] = role_counts.get(role, 0) + 1\n",
    "            msg_lengths.append((role, len(msg[\"value\"].split())))\n",
    "    \n",
    "    # Calculate role-specific statistics\n",
    "    role_length_stats = {}\n",
    "    for role in role_counts.keys():\n",
    "        lengths = [length for r, length in msg_lengths if r == role]\n",
    "        role_length_stats[role] = {\n",
    "            \"count\": len(lengths),\n",
    "            \"avg_length\": np.mean(lengths),\n",
    "            \"min_length\": min(lengths),\n",
    "            \"max_length\": max(lengths),\n",
    "        }\n",
    "    \n",
    "    # Print basic statistics\n",
    "    print(f\"Total conversations: {len(data)}\")\n",
    "    print(f\"Average messages per conversation: {np.mean(conversation_lengths):.1f}\")\n",
    "    print(\"\\nMessage counts by role:\")\n",
    "    for role, count in role_counts.items():\n",
    "        print(f\"  {role}: {count}\")\n",
    "    \n",
    "    print(\"\\nMessage length statistics by role (in words):\")\n",
    "    for role, stats in role_length_stats.items():\n",
    "        print(f\"  {role}: avg={stats['avg_length']:.1f}, min={stats['min_length']}, max={stats['max_length']}\")\n",
    "    \n",
    "    # Visualize message length by role\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    role_data = {}\n",
    "    for role, length in msg_lengths:\n",
    "        if role not in role_data:\n",
    "            role_data[role] = []\n",
    "        role_data[role].append(length)\n",
    "    \n",
    "    plt.boxplot([role_data[role] for role in role_data.keys()], labels=role_data.keys())\n",
    "    plt.title(\"Message Length by Role\")\n",
    "    plt.ylabel(\"Word Count\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Return statistics dict\n",
    "    return {\n",
    "        \"count\": len(data),\n",
    "        \"avg_conversation_length\": np.mean(conversation_lengths),\n",
    "        \"role_counts\": role_counts,\n",
    "        \"role_length_stats\": role_length_stats,\n",
    "    }\n",
    "\n",
    "# Analyze our sample data\n",
    "print(\"Alpaca Dataset Analysis:\")\n",
    "alpaca_stats = analyze_alpaca_dataset(cleaned_data)\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(\"ShareGPT Dataset Analysis:\")\n",
    "sharegpt_stats = analyze_sharegpt_dataset(sharegpt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Splitting into Training and Evaluation Sets\n",
    "\n",
    "Next, we'll split our dataset into training and evaluation sets. This is crucial for assessing the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data: List[Dict[str, Any]], eval_size: float = 0.1, seed: int = 42):\n",
    "    \"\"\"Split dataset into training and evaluation sets.\"\"\"\n",
    "    if len(data) < 10:  # For very small datasets\n",
    "        print(\"Warning: Dataset too small for meaningful split\")\n",
    "        return data, []\n",
    "    \n",
    "    train_data, eval_data = train_test_split(data, test_size=eval_size, random_state=seed)\n",
    "    print(f\"Split dataset into {len(train_data)} training examples and {len(eval_data)} evaluation examples\")\n",
    "    return train_data, eval_data\n",
    "\n",
    "# Split our cleaned data\n",
    "train_data, eval_data = split_dataset(cleaned_data)\n",
    "\n",
    "# Save split datasets\n",
    "train_path = data_dir / \"train.json\"\n",
    "eval_path = data_dir / \"eval.json\"\n",
    "\n",
    "with open(train_path, \"w\") as f:\n",
    "    json.dump(train_data, f, indent=2)\n",
    "    \n",
    "with open(eval_path, \"w\") as f:\n",
    "    json.dump(eval_data, f, indent=2)\n",
    "    \n",
    "print(f\"Saved training data to {train_path}\")\n",
    "print(f\"Saved evaluation data to {eval_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tokenization and Formatting\n",
    "\n",
    "Finally, let's see how the data will be tokenized and formatted for training. This step is handled by the framework, but it's useful to understand what's happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from src.utils.data_processing import process_alpaca_dataset\n",
    "\n",
    "# Load a tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\", trust_remote_code=True)\n",
    "    \n",
    "    # Ensure the tokenizer has padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Process a sample with the tokenizer to see how it will be formatted\n",
    "    example = train_data[0]\n",
    "    \n",
    "    # Apply chat template if available\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        # Format as a chat\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": example[\"instruction\"] + (\"\\n\" + example[\"input\"] if example.get(\"input\") else \"\")},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
    "        ]\n",
    "        \n",
    "        formatted_text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "        \n",
    "        print(\"Example formatted with chat template:\")\n",
    "        print(\"-\"*80)\n",
    "        print(formatted_text)\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Tokenize and show token count\n",
    "        tokens = tokenizer.encode(formatted_text)\n",
    "        print(f\"\\nToken count: {len(tokens)}\")\n",
    "    else:\n",
    "        print(\"Chat template not available for this tokenizer.\")\n",
    "        \n",
    "        # Fall back to simple format\n",
    "        if example.get(\"input\"):\n",
    "            formatted_text = f\"Instruction: {example['instruction']}\\nInput: {example['input']}\\nOutput: {example['output']}\"\n",
    "        else:\n",
    "            formatted_text = f\"Instruction: {example['instruction']}\\nOutput: {example['output']}\"\n",
    "            \n",
    "        print(\"Example formatted without chat template:\")\n",
    "        print(\"-\"*80)\n",
    "        print(formatted_text)\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Tokenize and show token count\n",
    "        tokens = tokenizer.encode(formatted_text)\n",
    "        print(f\"\\nToken count: {len(tokens)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load tokenizer: {e}\")\n",
    "    print(\"This step requires a valid model to be available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating a Configuration for Fine-tuning\n",
    "\n",
    "Now that we have prepared our dataset, let's create a configuration file for fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Create a configuration for fine-tuning\n",
    "config = {\n",
    "    \"model\": {\n",
    "        \"base_model\": \"meta-llama/Llama-3.1-8B-Instruct\",  # Replace with the model you want to use\n",
    "        \"tokenizer\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        \"load_in_4bit\": True,\n",
    "        \"trust_remote_code\": True,\n",
    "        \"use_flash_attention\": True,\n",
    "    },\n",
    "    \"fine_tuning\": {\n",
    "        \"method\": \"qlora\",\n",
    "        \"lora\": {\n",
    "            \"r\": 16,\n",
    "            \"alpha\": 32,\n",
    "            \"dropout\": 0.05,\n",
    "            \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            \"bias\": \"none\",\n",
    "        },\n",
    "        \"quantization\": {\n",
    "            \"bits\": 4,\n",
    "            \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
    "            \"bnb_4bit_quant_type\": \"nf4\",\n",
    "            \"bnb_4bit_use_double_quant\": True,\n",
    "        },\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"epochs\": 3,\n",
    "        \"micro_batch_size\": 1,\n",
    "        \"gradient_accumulation_steps\": 16,\n",
    "        \"learning_rate\": 2.0e-4,\n",
    "        \"lr_scheduler_type\": \"cosine\",\n",
    "        \"warmup_ratio\": 0.03,\n",
    "        \"max_grad_norm\": 0.3,\n",
    "        \"optimizer\": \"paged_adamw_8bit\",\n",
    "        \"weight_decay\": 0.001,\n",
    "        \"max_seq_length\": 4096,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"mixed_precision\": \"bf16\",\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"format\": \"alpaca\",\n",
    "        \"train_path\": str(train_path),\n",
    "        \"eval_path\": str(eval_path),\n",
    "        \"preprocessing\": {\n",
    "            \"add_eos_token\": True,\n",
    "            \"add_bos_token\": False,\n",
    "            \"use_chat_template\": True,\n",
    "        },\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"output_dir\": \"models/fine-tuned-model\",\n",
    "        \"logging_steps\": 10,\n",
    "        \"eval_steps\": 100,\n",
    "        \"save_steps\": 100,\n",
    "        \"save_total_limit\": 5,\n",
    "        \"push_to_hub\": False,\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"do_eval\": True,\n",
    "        \"eval_batch_size\": 8,\n",
    "        \"eval_strategy\": \"steps\",\n",
    "        \"eval_steps\": 100,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save the configuration\n",
    "config_path = project_root / \"config\" / \"data_prep_config.yaml\"\n",
    "os.makedirs(config_path.parent, exist_ok=True)\n",
    "\n",
    "with open(config_path, \"w\") as f:\n",
    "    yaml.dump(config, f, sort_keys=False)\n",
    "\n",
    "print(f\"Configuration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Data Processing Techniques\n",
    "\n",
    "For more advanced projects, you might need additional data processing techniques. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_length(data: List[Dict[str, Any]], min_words: int = 10, max_words: int = 1000, field: str = \"output\"):\n",
    "    \"\"\"Filter examples by the length of a specific field.\"\"\"\n",
    "    filtered_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        if field in item:\n",
    "            word_count = len(item[field].split())\n",
    "            if min_words <= word_count <= max_words:\n",
    "                filtered_data.append(item)\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "def augment_data(data: List[Dict[str, Any]], num_variants: int = 1):\n",
    "    \"\"\"Create simple variants of examples (placeholder for more sophisticated augmentation).\"\"\"\n",
    "    augmented_data = list(data)  # Start with original data\n",
    "    \n",
    "    # This is a placeholder for more sophisticated augmentation techniques\n",
    "    # In a real implementation, you might use:    \n",
    "    # - Paraphrasing\n",
    "    # - Back-translation\n",
    "    # - Word substitution\n",
    "    # - Using another LLM to generate variants\n",
    "    \n",
    "    print(\"Note: This is a placeholder for data augmentation.\")\n",
    "    print(\"In a real implementation, you would use techniques like paraphrasing or LLM generation.\")\n",
    "    \n",
    "    return augmented_data\n",
    "\n",
    "def balance_categories(data: List[Dict[str, Any]], category_field: str, max_per_category: Optional[int] = None):\n",
    "    \"\"\"Balance the dataset by limiting examples per category.\"\"\"\n",
    "    # Count examples per category\n",
    "    category_counts = {}\n",
    "    for item in data:\n",
    "        category = item.get(category_field, \"unknown\")\n",
    "        category_counts[category] = category_counts.get(category, 0) + 1\n",
    "    \n",
    "    # Determine maximum examples per category\n",
    "    if max_per_category is None:\n",
    "        # Use the minimum count as the maximum\n",
    "        max_per_category = min(category_counts.values())\n",
    "    \n",
    "    # Balance the dataset\n",
    "    balanced_data = []\n",
    "    category_current = {category: 0 for category in category_counts}\n",
    "    \n",
    "    for item in data:\n",
    "        category = item.get(category_field, \"unknown\")\n",
    "        if category_current[category] < max_per_category:\n",
    "            balanced_data.append(item)\n",
    "            category_current[category] += 1\n",
    "    \n",
    "    print(f\"Balanced dataset from {len(data)} to {len(balanced_data)} examples\")\n",
    "    print(\"Examples per category:\")\n",
    "    for category, count in category_current.items():\n",
    "        print(f\"  {category}: {count}\")\n",
    "    \n",
    "    return balanced_data\n",
    "\n",
    "# Example of using these advanced techniques\n",
    "# Note: These are placeholders for demonstration purposes\n",
    "print(\"Advanced data processing techniques:\")\n",
    "print(f\"Original data size: {len(cleaned_data)} examples\")\n",
    "\n",
    "# Filter by length (example)\n",
    "filtered_data = filter_by_length(cleaned_data, min_words=20, max_words=500)\n",
    "print(f\"After length filtering: {len(filtered_data)} examples\")\n",
    "\n",
    "# Note: The following are just placeholders and won't actually change the data\n",
    "# In a real implementation, you would implement these functions properly\n",
    "augmented_data = augment_data(filtered_data, num_variants=1)\n",
    "print(f\"After augmentation: {len(augmented_data)} examples\")\n",
    "\n",
    "# Balancing example (placeholder)\n",
    "# This assumes a 'category' field which doesn't exist in our sample data\n",
    "print(\"\\nCategory balancing would be used with a field specifying categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've covered the essential steps for preparing data for LLM fine-tuning:\n",
    "\n",
    "1. **Data collection and curation**: Understand the importance of high-quality, diverse examples\n",
    "2. **Data cleaning**: Remove duplicates, fix formatting issues, ensure quality\n",
    "3. **Format conversion**: Convert data to Alpaca or ShareGPT formats\n",
    "4. **Dataset analysis**: Understand the characteristics of your dataset\n",
    "5. **Dataset splitting**: Create proper training and evaluation sets\n",
    "6. **Tokenization and formatting**: See how data is prepared for model training\n",
    "7. **Configuration**: Create a configuration file for fine-tuning\n",
    "8. **Advanced techniques**: Explore more sophisticated data processing methods\n",
    "\n",
    "With these tools and techniques, you can prepare high-quality datasets for fine-tuning language models for your specific use cases.\n",
    "\n",
    "For the next steps, you can run the fine-tuning process using the configuration file we created:\n",
    "\n",
    "```bash\n",
    "python src/train.py --config config/data_prep_config.yaml\n",
    "```\n",
    "\n",
    "Or use the simplified interface:\n",
    "\n",
    "```bash\n",
    "python scripts/quick_finetune.py --model_name meta-llama/Llama-3.1-8B-Instruct --dataset_path data/train.json --eval_path data/eval.json --method qlora\n",
    "```\n",
    "\n",
    "Happy fine-tuning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
